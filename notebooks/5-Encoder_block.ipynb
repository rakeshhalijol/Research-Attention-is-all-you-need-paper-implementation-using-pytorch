{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4a4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 max_len: Annotated[int, \"It means how many no of words are there in a sequence\"],\n",
    "                 d_model: Annotated[int, \"It tells in how many dimension each and every word represents\"]) -> None:\n",
    "        super().__init__()\n",
    "        positional_encoder = torch.zeros(\n",
    "            max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(\n",
    "            1).float()  # (max_len, 1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float(\n",
    "        ) * (-math.log(10000.0) / d_model))  # (d_model//2,)\n",
    "\n",
    "        positional_encoder[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoder[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Shape it to (1, max_len, d_model) for broadcasting with input: (batch_size, seq_len, d_model)\n",
    "        positional_encoder = positional_encoder.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer(\"positional_encoder\", positional_encoder)\n",
    "\n",
    "    def forward(self, input_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_data: shape (batch_size, seq_len, d_model)\n",
    "        returns: same shape with positional encoding added\n",
    "        \"\"\"\n",
    "        seq_len = input_data.size(1)\n",
    "        return input_data + self.positional_encoder[:, :seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d5b7d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Annotated\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: Annotated[int, \"No of self attention needed\"],\n",
    "                 embed_dim: Annotated[int, \"dimension of each word\"],\n",
    "                 seq_length: Annotated[int, \"Length of sentence after padding\"],\n",
    "                 bias : Annotated[bool, \"Required bias during trining\"] = False,\n",
    "                 mask: Annotated[bool, \"normal MHA or masked MHA?\"] = False) -> None:\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim % num_heads != 0\"\n",
    "        self.num_heads = num_heads\n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.wv = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.output_projection = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.require_mask = mask\n",
    "        print(\"All parameters are set for multihead attention\")\n",
    "\n",
    "    def forward(self, batched_input_data:Annotated[torch.Tensor, \"batch of data from the input data\"]) -> torch.Tensor:\n",
    "        batch = batched_input_data.size(0)\n",
    "        q = self.wq(batched_input_data)\n",
    "        k = self.wk(batched_input_data)\n",
    "        v = self.wv(batched_input_data)\n",
    "\n",
    "        # Split the q, k, v(embed_dim) dimension as (num_head, embed_dim / num_head)\n",
    "        q = q.reshape(batch, self.seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.reshape(batch, self.seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.reshape(batch, self.seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "\n",
    "        # Calculate Attention\n",
    "        k_transpose = k.transpose(-2, -1)\n",
    "        score = (q @ k_transpose) / math.sqrt(self.head_dim)\n",
    "        mask = torch.triu(torch.ones(self.seq_length, self.seq_length), diagonal=1).bool() if self.require_mask else torch.zeros(self.seq_length, self.seq_length).bool()\n",
    "\n",
    "        # Anyhow broadcasting works no need of unsqeeze but its good practice to \n",
    "        # avoid broadcasting in Attentions, but clearly this step is optional\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "        score = score.masked_fill(mask, float(\"-inf\"))\n",
    "        attention_score = torch.softmax(score, dim=-1)\n",
    "        attention = attention_score @ v\n",
    "\n",
    "        # concat output of all heads\n",
    "        attention = attention.transpose(1, 2)\n",
    "        attention = attention.reshape(batch, self.seq_length, self.embed_dim)\n",
    "\n",
    "        # Since they are simple concatination to acutally mix all heads details we need a linear layer\n",
    "        \n",
    "        mha_output = self.output_projection(attention)\n",
    "        return mha_output\n",
    "    \n",
    "    # def __call__(self, batched_input_data:Annotated[torch.Tensor, \"batch of data from the input data\"]):\n",
    "    #     return self.forward(batched_input_data=batched_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a900dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, embed_dim:int, eps:float = 1e-9) -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(embed_dim)).float()\n",
    "        self.beta = nn.Parameter(torch.ones(embed_dim)).float()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input_data:torch.Tensor) -> torch.Tensor:\n",
    "        # Assume input dim(2, 3, 6)\n",
    "        mean = torch.mean(input_data, dim=-1, keepdim=True) # (2, 3, 1)\n",
    "        std = torch.std(input_data, dim=-1, keepdim=True) # (2, 3, 1)\n",
    "\n",
    "        # to normalize (2, 3, 6) - (2, 3, 1) = (2, 3, 6) due to broadcasting\n",
    "        normalized_input_data = (input_data - mean) / (std + self.eps)\n",
    "\n",
    "        # some weights do not require normalized output so alpha learnable parameter is introduced \n",
    "        return self.alpha * normalized_input_data + self.beta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ccee624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Annotated\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim:int, hidden_dim:int, dropout:float = 0.1,  bias:bool = True):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(embed_dim, hidden_dim, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.w2 = nn.Linear(hidden_dim, embed_dim, bias=bias)\n",
    "\n",
    "    def forward(self, input_data:torch.Tensor) -> torch.Tensor:\n",
    "        layer = self.w1(input_data)\n",
    "        output = self.relu(layer)\n",
    "        dropout = self.dropout(output)\n",
    "        layer = self.w2(dropout)\n",
    "        \n",
    "        return layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7851f1",
   "metadata": {},
   "source": [
    "### Suppose we have a dataset (rows = 100, seq_len = 10, embed_dim = 8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43c7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_data = torch.arange(100 * 10 * 8).reshape(100, 10, 8)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3678b643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 2\n",
    "seq_len = 10\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "batch_data = input_data[:batch]\n",
    "batch_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde7a40c",
   "metadata": {},
   "source": [
    "### Add postional encoding to all batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "701f9385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(seq_len, embed_dim)\n",
    "positional_encoded_batch_data = pe(batch_data)\n",
    "positional_encoded_batch_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30384735",
   "metadata": {},
   "source": [
    "### Now we have positional encoded data its time to send this data in MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc33271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters are set for multihead attention\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(num_heads=num_heads, seq_length=seq_len, embed_dim=embed_dim)\n",
    "mha_output_for_batched_data = mha(positional_encoded_batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f895c",
   "metadata": {},
   "source": [
    "### Apply Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee567596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_output_bach_data = mha_output_for_batched_data + positional_encoded_batch_data\n",
    "ln = LayerNormalization(embed_dim=embed_dim)\n",
    "layer_normalized_batch_data = ln(add_output_bach_data)\n",
    "\n",
    "layer_normalized_batch_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3615150",
   "metadata": {},
   "source": [
    "### Add this to a FFN to capture the non-linearity in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4de687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(embed_dim=embed_dim, hidden_dim=1024)\n",
    "ffn_batched_data = ffn(layer_normalized_batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b75a4a",
   "metadata": {},
   "source": [
    "### Get the encoder output after add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_layered_batch_data = ffn_batched_data + layer_normalized_batch_data\n",
    "encoder_output = ln(ffn_layered_batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86e176",
   "metadata": {},
   "source": [
    "### This is how encoder block works but since we need to do this Nx = 8 as mentioned in the paper lets implement this using loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39717f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx = 8\n",
    "pe = PositionalEncoding(10, 8)\n",
    "mha = MultiHeadAttention(num_heads=num_heads, embed_dim=embed_dim, seq_length=seq_len)\n",
    "ln1 = LayerNormalization(embed_dim=embed_dim)\n",
    "ln2 = LayerNormalization(embed_dim=embed_dim)\n",
    "ffn = FeedForward(embed_dim=embed_dim, hidden_dim=1024)\n",
    "\n",
    "# This is for birst batch of the data\n",
    "positional_encoded_batch_data = pe(batch_data)\n",
    "for i in range(Nx):\n",
    "    mha_output_for_batched_data = mha(positional_encoded_batch_data)\n",
    "    layer_normalized_batch_data = ln1(mha_output_for_batched_data + positional_encoded_batch_data)\n",
    "    ffn_batched_data = ffn(layer_normalized_batch_data)\n",
    "    encoder_output = ln2(ffn_batched_data + layer_normalized_batch_data)\n",
    "    positional_encoded_batch_data = encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58ee08",
   "metadata": {},
   "source": [
    "### we made a mistake\n",
    "- every encoder block in Nx should have separate MHA, FFN layer_norm parameters but we continued \n",
    "updating same weights.\n",
    "\n",
    "- to make separate parameters either we intalize and keep track of everything(unnecary code)\n",
    "\n",
    "- use nn.ModuleList to keep track of all parameter for each encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d39eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, seq_len:int, embed_dim:int, hidden_dim:int,num_heads:int, dropout = 0.1, bias:bool = False):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, embed_dim=embed_dim, seq_length=seq_len)\n",
    "        self.ln1 = LayerNormalization(embed_dim=embed_dim)\n",
    "        self.ln2 = LayerNormalization(embed_dim=embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim=embed_dim, hidden_dim=hidden_dim, dropout=dropout, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        # Multi-head attention + residual + norm\n",
    "        x_mha = self.mha(x)\n",
    "        x = self.ln1(x + self.dropout(x_mha))\n",
    "\n",
    "        # Feedforward + residual + norm\n",
    "        x_ffn = self.ffn(x)\n",
    "        x = self.ln2(x + self.dropout(x_ffn))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, Nx, embed_dim, seq_len, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(seq_len, embed_dim, ff_hidden_dim, num_heads, dropout)\n",
    "            for _ in range(Nx)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d470364b",
   "metadata": {},
   "source": [
    "### Test with above input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3057529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = torch.arange(100 * 10 * 8).reshape(100, 10, 8).float()\n",
    "batch = 2\n",
    "seq_len = 10\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "batch_data = input_data[:batch]\n",
    "batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb71125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters are set for multihead attention\n",
      "All parameters are set for multihead attention\n",
      "All parameters are set for multihead attention\n",
      "All parameters are set for multihead attention\n",
      "All parameters are set for multihead attention\n",
      "All parameters are set for multihead attention\n",
      "All parameters are set for multihead attention\n",
      "All parameters are set for multihead attention\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(Nx=8,\n",
    "                  embed_dim=embed_dim,\n",
    "                  seq_len=seq_len,\n",
    "                  ff_hidden_dim=1024,\n",
    "                  num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fac3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = encoder(batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459577c2",
   "metadata": {},
   "source": [
    "### And thats it we are successfully able to build the encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c8926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Attention is all you need",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
