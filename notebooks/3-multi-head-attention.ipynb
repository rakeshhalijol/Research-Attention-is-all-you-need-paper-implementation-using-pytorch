{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2785b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013680a1",
   "metadata": {},
   "source": [
    "### Define input of size(no of samples = 20, seq_length = 10, embed_dim = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc5e414e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 20\n",
    "seq_len = 10\n",
    "embed_dim = 8\n",
    "batch = 2\n",
    "no_head = 2\n",
    "input_data = torch.arange(n * seq_len * embed_dim).float().reshape(n, seq_len, embed_dim)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28862502",
   "metadata": {},
   "source": [
    "### Now intialize wq, wk, wv of shape (embed_dim, embed_dim) bcz the output must match the input_data dim (batch, seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e293d98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 8]), torch.Size([8, 8]), torch.Size([8, 8]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "wk = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "wv = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "wq.weight.shape, wv.weight.shape, wk.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7661104",
   "metadata": {},
   "source": [
    "### Calculate q, k, v vector with input_data later looking into whole data through loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b93803c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 8]), torch.Size([2, 10, 8]), torch.Size([2, 10, 8]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k , v = wq(input_data[:2]), wk(input_data[:2]), wv(input_data[:2])\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7fc2e",
   "metadata": {},
   "source": [
    "### split the data to both the heads(single self attention shouldn't get whole sequence rather a slice it should get another head get remaining)\n",
    "- i.e. each head should get (embed_dim / no_head) to capture different perspective of sentence\n",
    "- so q, k, v vector dimension should become (batch, no_head, seq_len, embed_dim / no_head) lets make it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ec80ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 10, 4]),\n",
       " torch.Size([2, 2, 10, 4]),\n",
       " torch.Size([2, 2, 10, 4]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert embed_dim / no_head, \"embed_dim / no_head should leave reminder 0.\"\n",
    "q = q.reshape(batch, no_head, seq_len, int(embed_dim / no_head))\n",
    "k = k.reshape(batch, no_head, seq_len, int(embed_dim / no_head))\n",
    "v = v.reshape(batch, no_head, seq_len, int(embed_dim / no_head))\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3ece55",
   "metadata": {},
   "source": [
    "### Calculate the attention score\n",
    "\n",
    "- Attention score = (Q . KT) / sqrt(d) \n",
    "- Attention  = softmax(Attention score) . V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb2ba263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0065e+01,  2.5342e+01,  1.2975e+01,  1.5621e+00],\n",
       "          [ 2.3878e+00, -3.3438e+00, -1.7794e+00,  2.0573e+00],\n",
       "          [ 9.8347e+00,  2.9392e+01,  1.5190e+01,  2.9186e-01],\n",
       "          [ 2.3956e+00, -3.4459e+00, -1.8458e+00,  2.0694e+00],\n",
       "          [ 9.8347e+00,  2.9392e+01,  1.5190e+01,  2.9186e-01],\n",
       "          [ 2.3956e+00, -3.4460e+00, -1.8459e+00,  2.0694e+00],\n",
       "          [ 9.8347e+00,  2.9392e+01,  1.5190e+01,  2.9186e-01],\n",
       "          [ 2.3956e+00, -3.4460e+00, -1.8459e+00,  2.0694e+00],\n",
       "          [ 9.8347e+00,  2.9392e+01,  1.5190e+01,  2.9186e-01],\n",
       "          [ 2.3956e+00, -3.4460e+00, -1.8459e+00,  2.0694e+00]],\n",
       "\n",
       "         [[ 1.9714e+01,  6.3272e+01,  3.1811e+01, -1.0734e+00],\n",
       "          [ 1.1811e+01,  3.6168e+01,  1.8514e+01,  1.8804e-02],\n",
       "          [ 1.9714e+01,  6.3272e+01,  3.1811e+01, -1.0734e+00],\n",
       "          [ 1.1811e+01,  3.6168e+01,  1.8514e+01,  1.8804e-02],\n",
       "          [ 1.9714e+01,  6.3272e+01,  3.1811e+01, -1.0734e+00],\n",
       "          [ 1.1811e+01,  3.6168e+01,  1.8514e+01,  1.8804e-02],\n",
       "          [ 1.9714e+01,  6.3272e+01,  3.1811e+01, -1.0734e+00],\n",
       "          [ 1.1811e+01,  3.6168e+01,  1.8514e+01,  1.8804e-02],\n",
       "          [ 1.9714e+01,  6.3272e+01,  3.1811e+01, -1.0734e+00],\n",
       "          [ 1.1811e+01,  3.6168e+01,  1.8514e+01,  1.8804e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.9594e+01,  9.7152e+01,  4.8432e+01, -2.4387e+00],\n",
       "          [ 2.1690e+01,  7.0048e+01,  3.5135e+01, -1.3465e+00],\n",
       "          [ 2.9594e+01,  9.7152e+01,  4.8432e+01, -2.4387e+00],\n",
       "          [ 2.1690e+01,  7.0048e+01,  3.5135e+01, -1.3465e+00],\n",
       "          [ 2.9594e+01,  9.7152e+01,  4.8432e+01, -2.4387e+00],\n",
       "          [ 2.1690e+01,  7.0048e+01,  3.5135e+01, -1.3465e+00],\n",
       "          [ 2.9594e+01,  9.7152e+01,  4.8432e+01, -2.4387e+00],\n",
       "          [ 2.1690e+01,  7.0048e+01,  3.5135e+01, -1.3465e+00],\n",
       "          [ 2.9594e+01,  9.7152e+01,  4.8432e+01, -2.4387e+00],\n",
       "          [ 2.1690e+01,  7.0048e+01,  3.5135e+01, -1.3465e+00]],\n",
       "\n",
       "         [[ 3.9473e+01,  1.3103e+02,  6.5053e+01, -3.8040e+00],\n",
       "          [ 3.1569e+01,  1.0393e+02,  5.1756e+01, -2.7118e+00],\n",
       "          [ 3.9473e+01,  1.3103e+02,  6.5053e+01, -3.8040e+00],\n",
       "          [ 3.1569e+01,  1.0393e+02,  5.1756e+01, -2.7118e+00],\n",
       "          [ 3.9473e+01,  1.3103e+02,  6.5053e+01, -3.8040e+00],\n",
       "          [ 3.1569e+01,  1.0393e+02,  5.1756e+01, -2.7118e+00],\n",
       "          [ 3.9473e+01,  1.3103e+02,  6.5053e+01, -3.8040e+00],\n",
       "          [ 3.1569e+01,  1.0393e+02,  5.1756e+01, -2.7118e+00],\n",
       "          [ 3.9473e+01,  1.3103e+02,  6.5053e+01, -3.8040e+00],\n",
       "          [ 3.1569e+01,  1.0393e+02,  5.1756e+01, -2.7118e+00]]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "K_transpose = k.transpose(-2, -1) # only make transpose to seq_length & embedding dimensio\n",
    "# K_transpose.shape\n",
    "head_dim = embed_dim // no_head\n",
    "# dim = -1 make sure it apply softmax along seq_length axis\n",
    "# (batch, num_head, seq_length, embed_dim) . (batch, num_head, embed_dim, seq_length) = (batch, num_head, seq_length, seq_length)\n",
    "attention_score = torch.softmax((q @ K_transpose) / math.sqrt(head_dim), dim=-1)\n",
    "# (batch, num_head, seq_length, seq_length) . (batch, num_head, seq_length,  head_dim) = (batch, num_head, seq_length,  head_dim)\n",
    "attention = attention_score @ v\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e915e0",
   "metadata": {},
   "source": [
    "### Combine all attention outputs\n",
    "- Step 1: attention = (batch, num_head, seq_length,  head_dim) now i want to make it (batch, seq_length, embed_dim)\n",
    "      \n",
    "        - convert (batch, num_head, seq_length,  head_dim) -> (batch, seq_length,  num_head,  head_dim)\n",
    "        - then reshape (batch, seq_length,  num_head,  head_dim) -> (batch, seq_length,  num_head * head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1248b1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 8])\n",
      "torch.Size([2, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "print(attention.shape)\n",
    "attention = attention.transpose(2, 1)\n",
    "attention = attention.reshape(batch, seq_len, embed_dim)\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d31238",
   "metadata": {},
   "source": [
    "### Final Linear Projection:- Mixing Information Across Heads\n",
    "Each attention head learns different relationships between tokens:\n",
    "\n",
    "- Head 1 might capture position-based attention.\n",
    "\n",
    "- Head 2 might focus on syntax.\n",
    "\n",
    "- Head 3 might capture coreference resolution.\n",
    "\n",
    "When you concatenate these heads → you get embed_dim = num_heads × head_dim. But it's just raw concatenation — there’s no interaction between heads yet.\n",
    "\n",
    " - The final linear projection mixes and combines the information from each head, similar to how an MLP might blend features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "363baf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension for output_proj is (embed_dim, embed_dim) bcz attention: (batch, seq_length,  embed_dim) . (embed_dim, embed_dim) = (batch, seq_length,  embed_dim)\n",
    "output_projection = nn.Linear(embed_dim, embed_dim)\n",
    "mha_output = output_projection(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e0374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Annotated\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: Annotated[int, \"No of self attention needed\"],\n",
    "                 embed_dim: Annotated[int, \"dimension of each word\"],\n",
    "                 seq_length: Annotated[int, \"Length of sentence after padding\"],\n",
    "                 bias : Annotated[bool, \"Required bias during trining\"] = False,\n",
    "                 mask: Annotated[bool, \"normal MHA or masked MHA?\"] = False) -> None:\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim % num_heads != 0\"\n",
    "        self.num_heads = num_heads\n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.wv = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.output_projection = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.require_mask = mask\n",
    "        print(\"All parameters are set for multihead attention\")\n",
    "\n",
    "    def forward(self, batched_input_data:Annotated[torch.Tensor, \"batch of data from the input data\"]) -> torch.Tensor:\n",
    "        batch = batched_input_data.size(0)\n",
    "        q = self.wq(batched_input_data)\n",
    "        k = self.wk(batched_input_data)\n",
    "        v = self.wv(batched_input_data)\n",
    "\n",
    "        # Split the q, k, v(embed_dim) dimension as (num_head, embed_dim / num_head)\n",
    "        q = q.reshape(batch, self.seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.reshape(batch, self.seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.reshape(batch, self.seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "\n",
    "        # Calculate Attention\n",
    "        k_transpose = k.transpose(-2, -1)\n",
    "        score = (q @ k_transpose) / math.sqrt(self.head_dim)\n",
    "        mask = torch.triu(torch.ones(self.seq_length, self.seq_length), diagonal=1).bool() if self.require_mask else torch.zeros(self.seq_length, self.seq_length).bool()\n",
    "\n",
    "        # Anyhow broadcasting works no need of unsqeeze but its good practice to \n",
    "        # avoid broadcasting in Attentions, but clearly this step is optional\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "        score = score.masked_fill(mask, float(\"-inf\"))\n",
    "        attention_score = torch.softmax(score, dim=-1)\n",
    "        attention = attention_score @ v\n",
    "\n",
    "        # concat output of all heads\n",
    "        attention = attention.transpose(1, 2)\n",
    "        attention = attention.reshape(batch, self.seq_length, self.embed_dim)\n",
    "\n",
    "        # Since they are simple concatination to acutally mix all heads details we need a linear layer\n",
    "        \n",
    "        mha_output = self.output_projection(attention)\n",
    "        return mha_output\n",
    "    \n",
    "    def __call__(self, batched_input_data:Annotated[torch.Tensor, \"batch of data from the input data\"]):\n",
    "        return self.forward(batched_input_data=batched_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bfda0970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters are set for multihead attention\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(2, 10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f14d455b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters are set for multihead attention\n",
      "torch.Size([2, 10, 8])\n",
      "tensor([[[-0.2354, -0.1877,  0.1168, -0.0770,  0.0946, -0.1002, -0.0696,\n",
      "           0.3571],\n",
      "         [-0.0869, -0.0991,  0.1638, -0.1026, -0.2098, -0.3868,  0.1530,\n",
      "           0.3094],\n",
      "         [-0.2698, -0.1626,  0.1131, -0.0524,  0.0779, -0.0901, -0.0244,\n",
      "           0.3530],\n",
      "         [-0.1712, -0.1059,  0.1276, -0.2048,  0.0497, -0.2817,  0.0845,\n",
      "           0.3163],\n",
      "         [-0.1566, -0.1520,  0.1457, -0.1357,  0.0571, -0.2097,  0.0937,\n",
      "           0.3678],\n",
      "         [-0.1077, -0.1232,  0.1558, -0.1313, -0.0856, -0.3185,  0.1598,\n",
      "           0.3364],\n",
      "         [-0.2102, -0.1894,  0.1443, -0.1102,  0.1576, -0.0961,  0.0029,\n",
      "           0.3992],\n",
      "         [-0.1075, -0.1111,  0.1154, -0.1638, -0.1535, -0.3867,  0.1337,\n",
      "           0.2721],\n",
      "         [-0.0326, -0.0999,  0.1546, -0.2402, -0.1516, -0.4747,  0.1807,\n",
      "           0.2834],\n",
      "         [-0.1586, -0.0969,  0.1165, -0.1488, -0.1341, -0.3506,  0.1090,\n",
      "           0.2774]],\n",
      "\n",
      "        [[-0.0848, -0.0345,  0.0096, -0.2946, -0.0433, -0.4787,  0.0382,\n",
      "           0.1813],\n",
      "         [-0.1007, -0.0671,  0.0139, -0.2933, -0.0305, -0.4440,  0.0748,\n",
      "           0.2031],\n",
      "         [-0.0768, -0.0594,  0.0025, -0.3309, -0.0638, -0.5024,  0.0951,\n",
      "           0.1777],\n",
      "         [-0.0675, -0.0170,  0.0224, -0.2625, -0.1160, -0.5118,  0.0602,\n",
      "           0.1742],\n",
      "         [-0.0331,  0.0258,  0.0391, -0.3020, -0.1018, -0.5700,  0.0685,\n",
      "           0.1657],\n",
      "         [-0.0842, -0.0298,  0.0055, -0.2888, -0.1343, -0.5231,  0.0914,\n",
      "           0.1670],\n",
      "         [-0.0570, -0.0351,  0.0064, -0.2819, -0.1248, -0.5245,  0.0692,\n",
      "           0.1651],\n",
      "         [-0.1030, -0.1101,  0.0008, -0.2986,  0.0229, -0.3957,  0.0577,\n",
      "           0.2175],\n",
      "         [-0.1025, -0.0590,  0.0242, -0.2694, -0.0367, -0.4342,  0.0722,\n",
      "           0.2109],\n",
      "         [-0.0279,  0.0467,  0.0440, -0.3026, -0.1551, -0.6100,  0.1354,\n",
      "           0.1621]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 2\n",
    "data = torch.randn(100, 10, 8)\n",
    "dataset = TensorDataset(data)\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "mha = MultiHeadAttention(embed_dim=8, num_heads=2, seq_length=10)\n",
    "\n",
    "for batch in loader:\n",
    "    x = batch[0]  # shape: (2, 10, 8)\n",
    "    out = mha(x)  # shape: (2, 10, 8)\n",
    "    print(out.shape)\n",
    "    print(out)\n",
    "    break  # test one batch for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a64a8def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10, 8])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 20\n",
    "seq_len = 10\n",
    "embed_dim = 8\n",
    "batch = 2\n",
    "no_head = 2\n",
    "input_data = torch.arange(n * seq_len * embed_dim).float().reshape(n, seq_len, embed_dim)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff78d90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(6, 6), diagonal=1).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b0e1aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.tensor([\n",
    "    [0.1, 0.3, 0.5, 0.7],\n",
    "    [0.2, 0.4, 0.6, 0.8],\n",
    "    [0.9, 0.1, 0.2, 0.3],\n",
    "    [0.3, 0.4, 0.5, 0.6]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c6f097f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(4, 4), diagonal=1).bool()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "62765a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000,   -inf,   -inf,   -inf],\n",
       "        [0.2000, 0.4000,   -inf,   -inf],\n",
       "        [0.9000, 0.1000, 0.2000,   -inf],\n",
       "        [0.3000, 0.4000, 0.5000, 0.6000]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.masked_fill(mask, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "549e5386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5, 5).bool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8850b0a",
   "metadata": {},
   "source": [
    "### But if you carefully observe transformers archetecture there are 3 types of attention 1) Multi-Head Attention 2) Masked-Multi head Attention 3) Cross-Multihead attention\n",
    "\n",
    "### But if you observe our implementation till now it only suppoerts MHA & MMHA\n",
    "\n",
    "- Now to integrate CMHA with our code we need to tweek changes in forward pass\n",
    "- forward method will accept 3 parameter (q_input, k_input = None, v_input = None) if k_input & v_input is None which means self attention else cross attention\n",
    "- also make changes in the dimension i.e. instead of seq_len we need to consider both qdim(output seq) and kdim(inp seq)\n",
    "\n",
    "### After making above changes final MultiHead attention class looks something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb68cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Annotated, Union\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: Annotated[int, \"No of self attention needed\"],\n",
    "                 embed_dim: Annotated[int, \"dimension of each word\"],\n",
    "                 bias: Annotated[bool, \"Required bias during trining\"] = False,) -> None:\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim % num_heads != 0\"\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.wv = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.output_projection = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                q_input: Annotated[torch.Tensor, \"batch of data from the input data\"],\n",
    "                k_input: Union[torch.Tensor, None] = None,\n",
    "                v_input: Union[torch.Tensor, None] = None,\n",
    "                mask: Annotated[bool, \"normal MHA or masked MHA?\"] = False) -> torch.Tensor:\n",
    "        batch = q_input.size(0)\n",
    "\n",
    "        # Below code make sures algorithm is self attention not cross attention\n",
    "        if k_input is None:\n",
    "            k_input = q_input\n",
    "        if v_input is None:\n",
    "            v_input = q_input\n",
    "\n",
    "        q = self.wq(q_input)\n",
    "        k = self.wk(k_input)\n",
    "        v = self.wv(v_input)\n",
    "\n",
    "        # Sequence length can be diffrent for input data and output data\n",
    "        T_q, T_k = q_input.size(1), k_input.size(1)\n",
    "\n",
    "        # Split the q, k, v(embed_dim) dimension as (num_head, embed_dim / num_head)\n",
    "        q = q.reshape(batch, T_q, self.num_heads,\n",
    "                      self.head_dim).transpose(1, 2)\n",
    "        k = k.reshape(batch, T_k, self.num_heads,\n",
    "                      self.head_dim).transpose(1, 2)\n",
    "        v = v.reshape(batch, T_k, self.num_heads,\n",
    "                      self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Calculate Attention\n",
    "        k_transpose = k.transpose(-2, -1)  # (b, k, d) (b, d, p) = (b, k, p)\n",
    "        score = (q @ k_transpose) / math.sqrt(self.head_dim)\n",
    "        mask = torch.triu(torch.ones(T_q, T_k), diagonal=1).bool(\n",
    "        ) if mask else torch.zeros(T_q, T_k).bool()\n",
    "\n",
    "        # Anyhow broadcasting works no need of unsqeeze but its good practice to\n",
    "        # avoid broadcasting in Attentions, but clearly this step is optional\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "        score = score.masked_fill(mask, float(\"-inf\"))\n",
    "        attention_score = torch.softmax(score, dim=-1)\n",
    "        attention = attention_score @ v\n",
    "\n",
    "        # concat output of all heads\n",
    "        attention = attention.transpose(1, 2)\n",
    "\n",
    "        # Attention should have sequence of length = output sequence length.\n",
    "        attention = attention.reshape(batch, T_q, self.embed_dim)\n",
    "\n",
    "        # Since they are simple concatination to acutally mix all heads details we need a linear layer\n",
    "\n",
    "        mha_output = self.output_projection(attention)\n",
    "        return mha_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09398524",
   "metadata": {},
   "source": [
    "### Testing Cross-Attention with simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411ed6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing Cross-Attention ---\n",
      "Query Input Shape (from decoder):      torch.Size([32, 15, 256])\n",
      "Key/Value Input Shape (from encoder):  torch.Size([32, 20, 256])\n",
      "\n",
      "Final Output Shape: torch.Size([32, 15, 256])\n",
      "\n",
      "Notice the output sequence length matches the query's sequence length.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup for the Test ---\n",
    "\n",
    "# 1. Define hyperparameters\n",
    "batch_size = 32\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "decoder_seq_len = 15  # The length of the target sequence\n",
    "encoder_seq_len = 20  # The length of the source sequence\n",
    "\n",
    "# 2. Instantiate the MultiHeadAttention layer\n",
    "mha_layer = MultiHeadAttention(num_heads=num_heads, embed_dim=embed_dim)\n",
    "\n",
    "# 3. Create the input data\n",
    "# This represents the decoder's input (e.g., the partially generated translation)\n",
    "decoder_input = torch.randn(batch_size, decoder_seq_len, embed_dim) \n",
    "\n",
    "# This represents the encoder's output, which provides the context\n",
    "encoder_output = torch.randn(batch_size, encoder_seq_len, embed_dim)\n",
    "\n",
    "print(\"--- Testing Cross-Attention ---\")\n",
    "print(f\"Query Input Shape (from decoder):      {decoder_input.shape}\")\n",
    "print(f\"Key/Value Input Shape (from encoder):  {encoder_output.shape}\\n\")\n",
    "\n",
    "# --- Perform the Cross-Attention ---\n",
    "\n",
    "# The decoder_input provides the queries.\n",
    "# The encoder_output provides the keys and values.\n",
    "output = mha_layer(q_input=decoder_input, k_input=encoder_output, v_input=encoder_output)\n",
    "\n",
    "print(f\"Final Output Shape: {output.shape}\")\n",
    "print(\"\\nNotice the output sequence length matches the query's sequence length.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa702f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Attention is all you need",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
