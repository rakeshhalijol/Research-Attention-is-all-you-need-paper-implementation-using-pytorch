{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed0851d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a386d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from typing_extensions import Annotated, List\n",
    "import torch.nn as nn \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dmodel:Annotated[int, \"Dimensions of the embeddings\"]) ->None:\n",
    "        super().__init__()\n",
    "        self.dmodel = dmodel\n",
    "\n",
    "\n",
    "    def forward(self, pos:Annotated[int, \"Position of the word\"]) -> torch.Tensor:\n",
    "        position_encoding_vector = torch.zeros(self.dmodel)\n",
    "        for i in range(0, self.dmodel, 2):\n",
    "            div_term = math.pow(10000, i / self.dmodel)\n",
    "            position_encoding_vector[i] = math.sin(pos / div_term)\n",
    "            if i + 1 < self.dmodel:\n",
    "                position_encoding_vector[i + 1] = math.cos(pos / div_term)\n",
    "            \n",
    "        return position_encoding_vector.unsqueeze(0)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3741d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8415, 0.5403, 0.0464, 0.9989, 0.0022, 1.0000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(6)\n",
    "pe.forward(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897ecb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 5, dtype=torch.float).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2646c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.0100])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.arange(0, 4, 2).float() * (-math.log(10000.0) / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa08b37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000, 100.0000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.arange(0, 4, 2).float() * math.log(10000) / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15608555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4,  5],\n",
       "         [ 6,  7,  8,  9, 10, 11],\n",
       "         [12, 13, 14, 15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20, 21, 22, 23],\n",
       "         [24, 25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34, 35]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(36).view(2, 3, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86c4e53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10).unsqueeze(1).float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c148afc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "         [27, 28, 29, 30, 31, 32, 33, 34, 35]],\n",
       "\n",
       "        [[ 9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
       "         [36, 37, 38, 39, 40, 41, 42, 43, 44]],\n",
       "\n",
       "        [[18, 19, 20, 21, 22, 23, 24, 25, 26],\n",
       "         [45, 46, 47, 48, 49, 50, 51, 52, 53]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(54).view(2, 3, 9).transpose(0, 1) #(3, 2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2cf68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 max_len: Annotated[int, \"It means how many no of words are there in a sequence\"],\n",
    "                 d_model: Annotated[int, \"It tells in how many dimension each and every word represents\"]) -> None:\n",
    "        super().__init__()\n",
    "        positional_encoder = torch.zeros(\n",
    "            max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(\n",
    "            1).float()  # (max_len, 1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float(\n",
    "        ) * (-math.log(10000.0) / d_model))  # (d_model//2,)\n",
    "\n",
    "        positional_encoder[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoder[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Shape it to (1, max_len, d_model) for broadcasting with input: (batch_size, seq_len, d_model)\n",
    "        positional_encoder = positional_encoder.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer(\"positional_encoder\", positional_encoder)\n",
    "\n",
    "    def forward(self, input_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        input_data: shape (batch_size, seq_len, d_model)\n",
    "        returns: same shape with positional encoding added\n",
    "        \"\"\"\n",
    "        seq_len = input_data.size(1)\n",
    "        return input_data + self.positional_encoder[:, :seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195c9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49857ff6",
   "metadata": {},
   "source": [
    "### Synthetic data (rows = 100, seq_len = 10, embed_dim = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03401595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = 100\n",
    "seq_len = 10\n",
    "embed_dim = 8\n",
    "batch = 2\n",
    "x = torch.arange(rows * seq_len * embed_dim).reshape(rows, seq_len, embed_dim)\n",
    "batch_data = x[:batch]\n",
    "batch_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff2c8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9161da05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.0000,   2.0000,   2.0000,   4.0000,   4.0000,   6.0000,   6.0000,\n",
       "            8.0000],\n",
       "         [  8.8415,   9.5403,  10.0998,  11.9950,  12.0100,  14.0000,  14.0010,\n",
       "           16.0000],\n",
       "         [ 16.9093,  16.5839,  18.1987,  19.9801,  20.0200,  21.9998,  22.0020,\n",
       "           24.0000],\n",
       "         [ 24.1411,  24.0100,  26.2955,  27.9553,  28.0300,  29.9995,  30.0030,\n",
       "           32.0000],\n",
       "         [ 31.2432,  32.3464,  34.3894,  35.9211,  36.0400,  37.9992,  38.0040,\n",
       "           40.0000],\n",
       "         [ 39.0411,  41.2837,  42.4794,  43.8776,  44.0500,  45.9987,  46.0050,\n",
       "           48.0000],\n",
       "         [ 47.7206,  49.9602,  50.5646,  51.8253,  52.0600,  53.9982,  54.0060,\n",
       "           56.0000],\n",
       "         [ 56.6570,  57.7539,  58.6442,  59.7648,  60.0699,  61.9976,  62.0070,\n",
       "           64.0000],\n",
       "         [ 64.9894,  64.8545,  66.7174,  67.6967,  68.0799,  69.9968,  70.0080,\n",
       "           72.0000],\n",
       "         [ 72.4121,  72.0889,  74.7833,  75.6216,  76.0899,  77.9960,  78.0090,\n",
       "           80.0000]],\n",
       "\n",
       "        [[ 80.0000,  82.0000,  82.0000,  84.0000,  84.0000,  86.0000,  86.0000,\n",
       "           88.0000],\n",
       "         [ 88.8415,  89.5403,  90.0998,  91.9950,  92.0100,  93.9999,  94.0010,\n",
       "           96.0000],\n",
       "         [ 96.9093,  96.5839,  98.1987,  99.9801, 100.0200, 101.9998, 102.0020,\n",
       "          104.0000],\n",
       "         [104.1411, 104.0100, 106.2955, 107.9553, 108.0300, 109.9995, 110.0030,\n",
       "          112.0000],\n",
       "         [111.2432, 112.3464, 114.3894, 115.9211, 116.0400, 117.9992, 118.0040,\n",
       "          120.0000],\n",
       "         [119.0411, 121.2837, 122.4794, 123.8776, 124.0500, 125.9987, 126.0050,\n",
       "          128.0000],\n",
       "         [127.7206, 129.9602, 130.5646, 131.8253, 132.0600, 133.9982, 134.0060,\n",
       "          136.0000],\n",
       "         [136.6570, 137.7539, 138.6442, 139.7648, 140.0699, 141.9976, 142.0070,\n",
       "          144.0000],\n",
       "         [144.9894, 144.8545, 146.7174, 147.6967, 148.0799, 149.9968, 150.0080,\n",
       "          152.0000],\n",
       "         [152.4121, 152.0889, 154.7833, 155.6216, 156.0899, 157.9960, 158.0090,\n",
       "          160.0000]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6904a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Attention is all you need",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
